{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e260a62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>owxbbpjpch.mp4</th>\n",
       "      <td>1</td>\n",
       "      <td>wynotylpnm.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpmyeepbep.mp4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fzvpbrzssi.mp4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>htorvhbcae.mp4</th>\n",
       "      <td>1</td>\n",
       "      <td>wclvkepakb.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fckxaqjbxk.mp4</th>\n",
       "      <td>1</td>\n",
       "      <td>vpmyeepbep.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                label        original\n",
       "owxbbpjpch.mp4      1  wynotylpnm.mp4\n",
       "vpmyeepbep.mp4      0             NaN\n",
       "fzvpbrzssi.mp4      0             NaN\n",
       "htorvhbcae.mp4      1  wclvkepakb.mp4\n",
       "fckxaqjbxk.mp4      1  vpmyeepbep.mp4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "# read the metadata.json file\n",
    "with open('metadata.json') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# create a dataframe with the metadata\n",
    "df = pd.DataFrame(metadata).T\n",
    "\n",
    "# remove the split column\n",
    "df = df.drop(columns=['split'])\n",
    "\n",
    "# create 1 and 0 labels from the label column\n",
    "df['label'] = df.label.apply(lambda x: 1 if x == 'FAKE' else 0)\n",
    "\n",
    "labels = df.label.values\n",
    "labels = tf.reshape(labels, (-1, 1))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f407f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n",
      "Tensorflow version: 2.4.1\n",
      "Keras version: 2.4.0\n",
      "Using Tesla V100-SXM2-32GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 11:24:55.019467: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-03-09 11:24:55.020659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:16:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-03-09 11:24:55.020722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-09 11:24:55.020744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-09 11:24:55.020805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-03-09 11:24:55.020820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-09 11:24:55.020830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-09 11:24:55.020841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-09 11:24:55.020852: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-09 11:24:55.020862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-09 11:24:55.022522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-03-09 11:24:55.022554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-09 11:24:55.022561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-03-09 11:24:55.022567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-03-09 11:24:55.024219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:16:00.0, compute capability: 7.0)\n",
      "2023-03-09 11:24:55.024339: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import Dense, Activation, Input, Input, Conv1D, Conv2D, MaxPooling1D\n",
    "from tensorflow.python.keras.layers import MaxPooling2D, Dense, Dropout, Activation, Flatten, InputLayer\n",
    "from tensorflow.python.keras.layers import concatenate, Reshape, Lambda, Add, Multiply, Average, Subtract\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# print Tensorflow and CUDA information\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
    "    name = details.get('device_name', 'Unknown GPU')\n",
    "    \n",
    "    print(f\"Using {name}\")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d658aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of real videos:  0\n",
      "Number of fake videos:  0\n"
     ]
    }
   ],
   "source": [
    "# print the number of real and fake videos\n",
    "print('Number of real videos: ', len(df[df.label == 'REAL']))\n",
    "print('Number of fake videos: ', len(df[df.label == 'FAKE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "082680fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "\n",
    "# get a list of all mp4 files\n",
    "filenames = [f for f in os.listdir(path) if f.endswith('.mp4')]\n",
    "\n",
    "# remove the .mp4 extension from the filenames\n",
    "filenames = [f[:-4] for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e675815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 300)\n"
     ]
    }
   ],
   "source": [
    "PROCESS_AUDIO = False\n",
    "\n",
    "if PROCESS_AUDIO:\n",
    "    import librosa\n",
    "    import moviepy.editor as mp\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    def save_audio_files():\n",
    "        for i, file in enumerate(filenames):\n",
    "            clip = mp.VideoFileClip(path + file + '.mp4')\n",
    "            clip.audio.write_audiofile(path + file + '.wav', verbose=False, logger=None)\n",
    "\n",
    "            # print the progress as a percentage\n",
    "            print(f'{i / len(filenames) * 100:.2f}%', end='\\r')\n",
    "        print('Audio files saved')\n",
    "\n",
    "\n",
    "    def save_audio_feature(file):\n",
    "        y, sr = librosa.load(path + file + '.wav')\n",
    "\n",
    "        frame_length = y.shape[0] // 300\n",
    "        \n",
    "        # discard trailing frames\n",
    "        y = y[:(frame_length * 300) - 1]\n",
    "\n",
    "        # this removes about half a frame of audio\n",
    "        # in the model this lost time will be shared between the frames\n",
    "        # this results in the error in each frame being very small\n",
    "\n",
    "        # calculate the mfcc with 32 features\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=32, n_fft=frame_length, hop_length=frame_length)\n",
    "\n",
    "        # save the mfcc\n",
    "        np.save(path + file + '.npy', mfcc)\n",
    "\n",
    "    def save_audio_feature_parallel():\n",
    "        with Pool(16) as p:\n",
    "            p.map(save_audio_feature, filenames)\n",
    "\n",
    "        print('Audio features saved')\n",
    "\n",
    "    save_audio_files()\n",
    "    save_audio_feature_parallel()\n",
    "\n",
    "# print the shape of a sample audio file\n",
    "print(np.load(path + filenames[0] + '.npy').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d31af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensorflow dataset\n",
    "dataset = tf.data.Dataset.list_files(path + '*.mp4')\n",
    "\n",
    "def get_video_asarray(file):    \n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(file.numpy().decode(\"utf-8\"))\n",
    "    for i in range(0, 30):\n",
    "        ret, frame = cap.read()\n",
    "        frame = preprocess_frame(frame)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.stack(frames, axis=0)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    # video is a numpy array of shape (height, width, 3)\n",
    "    # convert the video to grayscale\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # resize the video to 128x128\n",
    "    frame = cv2.resize(frame, (80, 45))\n",
    "    \n",
    "    frame = np.transpose(frame, (1, 0, 2))\n",
    "\n",
    "    # normalize the video\n",
    "    frame = frame / 255\n",
    "\n",
    "    return frame\n",
    "\n",
    "def get_audio_asarray(file):\n",
    "    file = file.numpy().decode(\"utf-8\")\n",
    "    \n",
    "    return np.load(file[:-4] + '.npy').T\n",
    "\n",
    "def preprocess_input(file):\n",
    "    # load the video\n",
    "    video = tf.py_function(get_video_asarray, [file], Tout=[tf.float32])\n",
    "    audio = tf.py_function(get_audio_asarray, [file], Tout=[tf.float32])\n",
    "    \n",
    "    audio = tf.expand_dims(audio, axis=-1)\n",
    "\n",
    "    return video, audio\n",
    "\n",
    "# Use map to apply the preprocess_input function to each file in the dataset\n",
    "dataset = dataset.map(preprocess_input)\n",
    "\n",
    "# Split the dataset into two separate datasets for video and audio\n",
    "video_dataset = dataset.map(lambda x, y: x)\n",
    "audio_dataset = dataset.map(lambda x, y: y)\n",
    "\n",
    "zipped_dataset = tf.data.Dataset.zip((video_dataset, audio_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf9fdd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video shape: (1, 30, 80, 45, 3)\n",
      "Audio shape: (1, 300, 32, 1)\n",
      "CPU times: user 815 ms, sys: 5.01 ms, total: 820 ms\n",
      "Wall time: 970 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocess the video and audio for the first video\n",
    "for video, audio in zipped_dataset.take(1):\n",
    "    print(f'Video shape: {video.shape}')\n",
    "    print(f'Audio shape: {audio.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0b903630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "video_input (InputLayer)        [(None, 30, 80, 45,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "audio_input (InputLayer)        [(None, 300, 32, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_44 (Conv3D)              (None, 26, 77, 43, 1 2896        video_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 298, 30, 16)  160         audio_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_30 (MaxPooling3D) (None, 13, 25, 14, 1 0           conv3d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 149, 15, 16)  0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_45 (Conv3D)              (None, 11, 22, 12, 3 18464       max_pooling3d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 147, 13, 32)  4640        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 300, 29, 16)  80          audio_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_31 (MaxPooling3D) (None, 5, 5, 4, 32)  0           conv3d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 73, 6, 32)    0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 300, 22, 32)  4128        conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_46 (Conv3D)              (None, 3, 2, 2, 64)  73792       max_pooling3d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 71, 4, 64)    18496       max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 300, 11, 64)  24640       conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 768)          0           conv3d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 18176)        0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 211200)       0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 230144)       0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128)          29458560    concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 64)           8256        dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            65          dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 29,614,177\n",
      "Trainable params: 29,614,177\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv3D, Conv2D, Conv1D, Flatten, Dense, concatenate, MaxPooling3D, MaxPooling2D\n",
    "\n",
    "# Define the text input shape\n",
    "video_input = Input(shape=(30, 80, 45, 3), name='video_input')\n",
    "\n",
    "# Define the image input shape\n",
    "audio_input = Input(shape=(300, 32, 1), name='audio_input')\n",
    "\n",
    "# 3D convolutional layers for video input\n",
    "x = Conv3D(16, kernel_size=(5,4,3), activation='relu')(video_input)\n",
    "x = MaxPooling3D(pool_size=(2,3,3))(x)\n",
    "x = Conv3D(32, kernel_size=(3,4,3), activation='relu')(x)\n",
    "x = MaxPooling3D(pool_size=(2,4,3))(x)\n",
    "x = Conv3D(64, kernel_size=(3,4,3), activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# 2D convolutional layers for audio input\n",
    "y = Conv2D(16, kernel_size=(3,3), activation='relu')(audio_input)\n",
    "y = MaxPooling2D(pool_size=(2,2))(y)\n",
    "y = Conv2D(32, kernel_size=(3,3), activation='relu')(y)\n",
    "y = MaxPooling2D(pool_size=(2,2))(y)\n",
    "y = Conv2D(64, kernel_size=(3,3), activation='relu')(y)\n",
    "y = Flatten()(y)\n",
    "\n",
    "# 1D convolutional layers for both inputs\n",
    "z = Conv1D(16, kernel_size=4, activation='relu')(audio_input)\n",
    "z = Conv1D(32, kernel_size=8, activation='relu')(z)\n",
    "z = Conv1D(64, kernel_size=12, activation='relu')(z)\n",
    "z = Flatten()(z)\n",
    "\n",
    "# Concatenate the outputs from all three convolutional layers\n",
    "merged = concatenate([x, y, z])\n",
    "\n",
    "# Dense layers for classification\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[video_input, audio_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf527c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1334/1334 [==============================] - 888s 665ms/step - loss: 3.1323 - accuracy: 0.9092\n",
      "Epoch 2/10\n",
      "1334/1334 [==============================] - 860s 645ms/step - loss: 0.2926 - accuracy: 0.9298\n",
      "Epoch 3/10\n",
      " 221/1334 [===>..........................] - ETA: 11:58 - loss: 0.3902 - accuracy: 0.8895"
     ]
    }
   ],
   "source": [
    "labels_t = tf.data.Dataset.from_tensor_slices(labels)\n",
    "dataset_with_labels = tf.data.Dataset.zip((zipped_dataset, labels_t))\n",
    "\n",
    "model.fit(dataset_with_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c8a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
